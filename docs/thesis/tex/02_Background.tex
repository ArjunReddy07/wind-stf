\chapter{Background}
In this chapter, we first present the different settings, approaches, and performance metrics used in general spatio-temporal forecasting problems.
We then describe fundamental aspects of wind power generation underlying this work.

\section{Spatio-Temporal Forecasting}
In this section, we present the spatio-temporal forecasting problem, how it is approached, and how forecasting models can be assessed and compared.

\subsection{Problem statement}
In this work, we categorize spatio-temporal forecasting problems according to (1) the degree of dependency among sensors and (2) the stationarity of sensors locations.
The term sensor is used here in the abstract sense of a stochastic data generation process, which could be physically represented by an actual sensor measuring a variable of interest in a particular phenomenon.

% Problem statement for the most reduced/simplest ST forecasting setting (1), to the most general/complex one (2).
In a first regime, referred here as regime I, sensors are fixed in space, with negligible dependencies among them.
The uncertainty about the state of a sensor cannot be reduced by knowing the states of its neighboring sensors.
As a consequence, using a single model to represent the different sensors is expected to present no advantage over modeling every sensor independently.
Characteristic of this regime is also the covariance matrix for the different sensors being both diagonal and invariant in time. 

In regime II, sensors are also fixed in space, but this time with significant dependencies among them.
The uncertainty about the state of a sensor can be reduced by knowing the states of its neighboring sensors.
In other words, uncertainties among sensors are coupled.
Modeling sensors together could be potentially beneficial in such case. 
Besides, the covariance matrix is expected to be non-diagonal but still invariant in time. 

Finally, in regime III, dependent sensors move in space.
Dependencies across sensors should hence also change over time, and a corresponding time-dependent covariance matrix is expected to follow. 
Again, models representing multiple sensors could make use of this and outperform models for single sensors. 

\subsection{Conventional Approaches}
In this work, we refer as conventional approaches what is in the literature often referred as Time Series \footnote{A time series is defined as a stochastic process ${..., X_{t-1}, X_{t}, X_{t+1}, ...}$ consisting of random variables indexed by time index $t$. The stochastic behavior ${X_t}$ is described by $p(x_{t1}, x_{t2}, ..., x_{tm})$, i.e.,the PDF (or PMF) for all finite collections of time indexes ${(t_1, t_2, ..., t_m), m<\infty}$ \cite{kempthorne2013s0196}. } approaches.
Their formulations were motivated by forecasting problems in which time was the single independent variable.
The hallmark of conventional forecasting approaches is their reliance on the well-developed theory for describing stationary random processes. 
There are two general ways of describing (modeling) a generic time series: (1) the Exponential Smoothing (ES) framework, and (2) the Auto-Regressive Integrated Moving Average (ARIMA) framework \cite{brockwell1991methods}. 

\subsubsection{Exponential Smoothing Framework}
In the ES framework, time series are modelled as a superposition of three components: trend ($m_t$), seasonal ($s_t$), and random noise ($Y_t$).
This is known as the Classical Decomposition (\autoref{classical-decomposition}).

\begin{equation}\label{classical-decomposition}
    X_t = m_t + s_t + Y_t
\end{equation}

The underlying principle is to apply a filter to $X_t$ that smooths out the noise component $Y_t$, allowing $m_t$ and $s_t$ to be estimated and extracted.
Techniques within this framework differ by (a) the filter, (b) the assumptions on and preprocessing of $X_t$. 
In fact, the simplicity of models based on ES and their success in temporal forecasting problems have made this framework the default choice in the industry for such settings \cite{holt2004forecasting}.
We describe two methods as examples: (1) the least squares, (2) the exponential smoothing method.
Both assume non-seasonality of $X_t$ (i.e.,$s_t=0$), meaning a deseasoning of the time series is typically required as a preprocessing step.

In the least squares method, $m_t$ is first approximated by a parametric family of functions (e.g. $m_t = a_0 + a_1 t + a_2 t^2$).
The parameters are then estimated via the the minimization of the squared errors $\Sigma_t (x_t - m_t)^2$. 

In the exponential smoothing method, a pre-defined $a \in [0, 1]$ is used for the estimated trend $\hat{m}_t$ by \autoref{exponential-smoothing}.
\begin{align}\label{exponential-smoothing}
    \hat{m}_t &= a X_t + (1-a) \hat{m}_{t-1}, t=1, ..., n
    \hat{m}_1 &= X_1
\end{align}

The resulting expression of $\hat{m}_t$ in terms of the past measurements $X_t, X_{t-1}, ...$, motivates the name of this method:
\begin{equation}\label{exponential-smoothing2}
    \hat{m}_t = \sum_{j=0}{t-2} a (1-a)^j X_{t-j} + (1-a)^{t-1} X_1 \hat{m}_{t-1}, 
\end{equation}
i.e.,$\hat{m}_t$ is a weighted moving average of the past measurements $X_t, X_{t-1}, ...$, with weigths decreasing exponentially.

So far, the exponential smoothing framework has been presented in its univariate version.
Its multivariate version is the Vector Exponential Smoothing (VES) framework.  
While ES-based models can be used to properly address regime I forecasting problems, VES-based models can incorporate cross-sensors dependencies into the covariance matrix to model sensors under regime I, II or III.

\subsubsection{ARIMA Framework}
% ARIMA framework (univariate version: ARIMA; multivariate VAR, VARIMA)
First proposed by \cite{box1970time} (and hence often referred as Box-Jenkins Methods), the ARIMA framework relies on differencing to achieve stationarity.
Differencing operators, are recursively applied to the data ${x_t}$ until the resulting observations are approximatelly stationary. \cite{brockwell1991methods}
For $k$ recursions, the corresponding operator is referred as $\nabla^k(\cdot)$.
As an instance, for k=1: $\nabla X_t = X_{t} - X_{t-1}$.

The framework is named after the ARIMA model, described by \autoref{arima}, with the autoregressive operator $\phi_k$, moving average operator $\theta_k$ (both of order $k$), and innovation (white noise) at time index $t$ $a_t$.
\begin{align}\label{arima}
    x_t = & \phi_1 x_t + ... + \phi_{p+d} x_{t-p-d}
          & - \theta_{1} a_{t-1} - ... - \theta{q} a_{t-q} + a_t 
\end{align}
The first line of \autoref{arima} corresponds to the autoregressive (AR) component of ARIMA, in which $x_t$ is represented as a linear regression of its preceding values $x_{t-1}, ..., x_{t-p-d}$.

Important nonlinear methods are included in this framework, such as ARCH and GARCH, in which the innovation term is modelled by respectively by an AR model and by an ARMA model.
The multivariate version of ARIMA is the Vector-ARIMA (VARIMA).
Like its counterpart from the Exponential Smoothing, VARIMA models make use of a covariance matrix to incorporate cross-sensors dependencies for the higher coupling regimes II and III.

\subsection{Machine Learning-based Approaches}
Machine Learning approaches rely on (1) definition of relatively general architectures and (2) finding a configuration of parameter values in the given architecture that minimizes the expectation of some loss function.
As the loss function represents a discrepancy between predictions and ground truth, this optimization process leads to a model that can be used to predict system behavior given a configuration for inputs values.
The optimization process itself is typically performed by a gradient-based algorithm. \cite{goodfellow2016deep}

In the context of univariate temporal forecasting (i.e.,regime I), the performance of Machine Learning algorithms was considered by some to be very limited reliability and usefulness \cite{makridakis2000m3}.
However, \cite{bengio2019nbeats} recently demonstrated that a "pure" Deep Learning approach could not only (1) consistently outperform conventional ones, but also (2) be less reliant on manual tuning and (3) be made interpretable in both final and intermediate outputs.
Until then, top-performing ML-based models were either a result of a combination or hybridization with conventional methods.
Earlier approaches relied on ML-TS Combinations, in which outputs from statistical engines were used as features for ML algorithms.
Later, TS models had their parameters optimized via gradient-descent and stacked with a Recurrent Neural Network (RNN) to form a hybrid model \cite{smyl2020esrnn}.

Outside of regime I, ST forecasting problems were already successfully addressed by Deep Learning approaches, such as in forecasting traffic \cite{liu2017dcrnn}, ride-hailing demand \cite{li2019stgcn} and electrical power demand \cite{toubeau2018blstm}.
Most of these approaches relied on RNN architectures, eventually combined with a CNN architecture.
More recently, approaches that model the Spatio-Temporal dependencies over a non-Euclidean space in a graph representation have been proposed and currently represent the state-of-the-art for ST forecasting problems \cite{zhang2019graphwavenet}.

\subsection{Forecasting Performance Metrics}
Different quantities can be used for assessing models forecasting performance.
Some of the most popular are $MAE$ (Mean Absolute Error, \autoref{mae}), $MAPE$ (Mean Absolute Percentual Error, \autoref{mape}), $RMSE$ (Root Mean Squared Error, \autoref{rmse}).
As they expose different qualities of performance, combining a reasonable number of metrics can be advisable.  
\begin{equation}\label{mae}
    MAE(\bm{\hat{X}}^{(t+i):(t+T)}; \bm{\Theta}) = \frac{1}{T N D} \Sigma_{i=1}^{T} \Sigma_{j=1}^{N} \Sigma_{k=1}^{D} | \bm{\hat{X}}^{(t+i)}_{jk} - X_{jk}^{(t+i)} |
\end{equation}

\begin{equation}\label{mape}
    MAPE(\bm{\hat{X}}^{(t+i):(t+T)}; \bm{\Theta}) = \frac{100}{T N D} \Sigma_{i=1}^{T} \Sigma_{j=1}^{N} \Sigma_{k=1}^{D} \frac{ | \bm{\hat{X}}^{(t+i)}_{jk} - X_{jk}^{(t+i)} | }{ |X_{jk}^{t+i}| }
\end{equation}

\begin{equation}\label{rmse}
    RMSE(\bm{\hat{X}}^{(t+i):(t+T)}; \bm{\Theta}) = \sqrt{ \frac{1}{T N D} \Sigma_{i=1}^{T} \Sigma_{j=1}^{N} \Sigma_{k=1}^{D} (\bm{\hat{X}}^{(t+i)}_{jk} - X_{jk}^{(t+i)})^2 }
\end{equation}


\section{Wind Power Generation}
% Wind power: variables affecting available energy
% Wind power: variables affecting conversion of (part of) available energy effective generated power
% Economics (costs)
% Economics (market)
% key parameters in wind power generation (capacity factor), intermittent nature & why forecasting is important (potential)
% Power generation measurements in a single turbine, as well as its aggregation by farm or by district as a stochastic process
% This stochastic process can be cast as a Time Series