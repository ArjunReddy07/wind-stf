{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Dict, List\n",
    "import time \n",
    "from datetime import datetime  \n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infer_scaled = df = pd.read_hdf(\n",
    "    path_or_buf='../data/05_model_input/df_infer_scaled.hdf', \n",
    "    key='df_infer_scaled'\n",
    ")\n",
    "\n",
    "with open('../data/05_model_input/splits_positions.pkl/2020-10-01T14.00.54.003Z/splits_positions.pkl', 'rb') as pkl_file:\n",
    "    splits_positions = pickle.load(pkl_file)\n",
    "    \n",
    "with open(r'../conf/base/parameters.yml') as file:\n",
    "    params = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    \n",
    "modeling = params['modeling']\n",
    "cv_params = params['cv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infer_scaled.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_infer_scaled[['DEF0C']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_infer_scaled.loc['2013-01-01': '2015-12-22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = slice('2013-01-01', '2015-12-22')\n",
    "df_infer_scaled[train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining CV Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pass_1': {'train_idx': [0, 730], 'test_idx': [730, 737]},\n",
       " 'pass_2': {'train_idx': [0, 816], 'test_idx': [816, 823]},\n",
       " 'pass_3': {'train_idx': [0, 902], 'test_idx': [902, 909]}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size_first_pass = cv_params['window_size_first_pass']\n",
    "window_size_last_pass = cv_params['window_size_last_pass']\n",
    "if window_size_last_pass == 'complete inference window':\n",
    "    window_size_last_pass = len(df)\n",
    "n_passes = cv_params['n_passes']\n",
    "forecasting_window_size = cv_params['forecasting_window_size']\n",
    "\n",
    "cv_splits_dict = {}\n",
    "window_size_increment = int((window_size_last_pass - window_size_first_pass) / (n_passes - 1))\n",
    "for p in range(n_passes):\n",
    "    pass_id = 'pass_' + str(p + 1)\n",
    "    cv_splits_dict[pass_id] = {\n",
    "        'train_idx': [\n",
    "            0,\n",
    "            window_size_first_pass + p * window_size_increment\n",
    "        ],\n",
    "        'test_idx': [\n",
    "            window_size_first_pass + p * window_size_increment,\n",
    "            window_size_first_pass + p * window_size_increment + forecasting_window_size,\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "cv_splits_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size_first_pass = cv_params['window_size_first_pass']\n",
    "window_size_last_pass = cv_params['window_size_last_pass']\n",
    "if window_size_last_pass == 'complete inference window':\n",
    "    window_size_last_pass = len(df)\n",
    "n_passes = cv_params['n_passes']\n",
    "forecasting_window_size = cv_params['forecasting_window_size']\n",
    "\n",
    "cv_splits_dict = {}\n",
    "window_size_increment = int((window_size_last_pass - window_size_first_pass) / (n_passes - 1))\n",
    "for p in range(n_passes):\n",
    "    pass_id = 'pass_' + str(p + 1)\n",
    "    cv_splits_dict[pass_id] = {\n",
    "        'train': slice(\n",
    "            df.index[0],\n",
    "            df.index[ window_size_first_pass + p * window_size_increment ]\n",
    "        ),\n",
    "        'val': slice(\n",
    "            df.index[ window_size_first_pass + p * window_size_increment ],\n",
    "            df.index[ window_size_first_pass + p * window_size_increment + forecasting_window_size ]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "cv_splits_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all vars we don't want to model\n",
    "targets = modeling['targets']\n",
    "df = df[targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = slice( \n",
    "    splits_positions['pass_1']['train_idx'][0],\n",
    "    splits_positions['pass_1']['train_idx'][1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = slice(\n",
    "    df.index[0],\n",
    "    df.index[730]\n",
    ")\n",
    "\n",
    "df[train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Split-Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df: pd.DataFrame, modeling):\n",
    "    train = slice(\n",
    "        modeling['train_window']['start'],\n",
    "        modeling['train_window']['end']\n",
    "    )\n",
    "    \n",
    "    test = slice(\n",
    "        modeling['test_window']['start'],\n",
    "        modeling['test_window']['end']\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'df_train': df[train],\n",
    "        'df_test': df[test]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeStrictlyPositive(TransformerMixin, BaseEstimator):\n",
    "    '''Add constant to variable so that it only assumes positive values.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.offset_ = X.min(axis=0)\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X + abs(self.offset_)\n",
    "    \n",
    "    def inverse_transform(self, X, y=None):\n",
    "        return X - abs(self.offset_)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMERS = {\n",
    "    'get_quantile_equivalent_normal_dist': QuantileTransformer(\n",
    "                                                output_distribution='normal', \n",
    "                                                random_state=0,\n",
    "                                            ),\n",
    "    'make_strictly_positive': MakeStrictlyPositive(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = make_pipeline(\n",
    "    QuantileTransformer(\n",
    "        output_distribution='normal', \n",
    "        random_state=0,\n",
    "    ),\n",
    "    MakeStrictlyPositive(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = make_pipeline(\n",
    "    *[ TRANSFORMERS[ step ] for step in modeling['preprocessing'] ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = split_data(df_spatiotemporal, modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train_test_split['df_train']\n",
    "\n",
    "df_train['temporal'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_preprocessed = df_train.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = preprocessing_pipeline.fit(\n",
    "    df_train_copy['temporal']\n",
    ")\n",
    "\n",
    "df_train_preprocessed['temporal'].update(\n",
    "    preprocessing_pipeline.transform(\n",
    "        df_train_copy['temporal']\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_train_copy['temporal'] < 0).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_params = {\n",
    "    'cv_type': 'expanding_windows',\n",
    "    'window_size_first_pass': 365,\n",
    "    'window_size_last_pass': 540,\n",
    "    'n_passes': 3,\n",
    "    'forecasting_window_size': 7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_cvsplits(cv_pars: Dict) -> Dict[str, Any]:  # Dict[str, List[pd.date_range, List[str]]]:\n",
    "    \"\"\"\n",
    "    Example of Cross-Validation Splits Dictionary:\n",
    "\n",
    "    cv_splits_dict = {\n",
    "        'pass_1': {\n",
    "            'train_idx': [0, 365],\n",
    "            'eval_idx': [365, 465],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    :param window_size_first_pass:\n",
    "    :param window_size_last_pass:\n",
    "    :param n_passes:\n",
    "    :param forecasting_window_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    window_size_first_pass = cv_pars['window_size_first_pass']\n",
    "    window_size_last_pass = cv_pars['window_size_last_pass']\n",
    "    n_passes = cv_pars['n_passes']\n",
    "    forecasting_window_size = cv_pars['forecasting_window_size']\n",
    "\n",
    "    cv_splits_dict = {}\n",
    "    window_size_increment = int( (window_size_last_pass - window_size_first_pass) / (n_passes-1) )\n",
    "    for p in range(n_passes):\n",
    "        pass_id = 'pass_' + str(p + 1)\n",
    "        cv_splits_dict[pass_id] = {\n",
    "                'train_idx': [\n",
    "                    0,\n",
    "                    window_size_first_pass + p * window_size_increment\n",
    "                ],\n",
    "                'eval_idx': [\n",
    "                    window_size_first_pass + p * window_size_increment,\n",
    "                    window_size_first_pass + p * window_size_increment + forecasting_window_size,\n",
    "                ],\n",
    "        }\n",
    "    return cv_splits_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_train_val(df: pd.DataFrame, cv_splits_dict: dict, pass_id: str):\n",
    "    train_idx_start = cv_splits_dict[pass_id]['train_idx'][0]\n",
    "    train_idx_end = cv_splits_dict[pass_id]['train_idx'][1]\n",
    "\n",
    "    test_idx_start = cv_splits_dict[pass_id]['test_idx'][0]\n",
    "    test_idx_end = cv_splits_dict[pass_id]['test_idx'][1]\n",
    "\n",
    "    return {\n",
    "        'train': df.iloc[train_idx_start:train_idx_end, :],\n",
    "        'val': df.iloc[test_idx_start:test_idx_end, :],\n",
    "    }\n",
    "\n",
    "\n",
    "class ForecastingModel:\n",
    "    def __init__(self, y_train, modeling):\n",
    "        \n",
    "        # model artifacts (metadata) for training\n",
    "        self.modeling_settings = modeling\n",
    "        self.y_train_info = y_train.info()\n",
    "        self.y_train_columns = y_train.columns\n",
    "        \n",
    "        self.hyperpars = modeling['hyperpars']\n",
    "        \n",
    "        self.targets_list = self.modeling['target_timeseries']\n",
    "        if self.targets_list == 'all_available':\n",
    "            self.targets_list = y_train.columns \n",
    "        \n",
    "        y_train_ = y_train  # i.e. all districts at once (spatio-temporal)\n",
    "        if self.modeling_settings['mode'] == 'temporal':  # i.e. districtwise\n",
    "            y_train_ = y_train['temporal']\n",
    "\n",
    "            \n",
    "    def fit(self, district=None):      \n",
    "        self.datetime_start = datetime.now()\n",
    "        time_start = time.time()\n",
    "        \n",
    "        if self.modeling_settings['approach'] == 'HW-ES':\n",
    "            self.submodels_ = { \n",
    "                district: ExponentialSmoothing( \n",
    "                    endog=y_train_[district], \n",
    "                    *self.hyperpars,\n",
    "                ).fit() for district in self.targets_list \n",
    "            } \n",
    "\n",
    "        elif self.modeling_settings['approach'] == 'RNN-ES':\n",
    "            self.model_ = None\n",
    "\n",
    "        elif self.modeling_settings['approach'] == 'GWNet':\n",
    "            self.model_ = None\n",
    "        \n",
    "        else: \n",
    "            return NotImplementedError(f'Invalid modeling approach {self.modeling_settings[\"approach\"]}')\n",
    "        \n",
    "        self.training_duration = format( time.time() - time_start, \"2.00E\" ) + ' secs' \n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def predict(self, start, end, transformer):               \n",
    "        y_hat = pd.DataFrame(\n",
    "            data=None,\n",
    "            columns=self.y_train_columns,\n",
    "        )\n",
    "        \n",
    "        if self.modeling_settings['mode'] == 'temporal': # i.e. districtwise\n",
    "            y_hat.update(\n",
    "                data = {\n",
    "                    self.submodels_[district].predict(\n",
    "                        start=start,\n",
    "                        end=start,)\n",
    "                for district in self.y_train_columns}, \n",
    "                copy=False,\n",
    "            )\n",
    "        else: \n",
    "            y_hat.update(\n",
    "                data = {\n",
    "                    self.model_.predict(\n",
    "                        start=start,\n",
    "                        end=start,\n",
    "                    )\n",
    "                }\n",
    "                copy=False,\n",
    "            )\n",
    "        \n",
    "        y_hat_unscaled = transformer.inverse_transform(y_hat)\n",
    "        return y_hat_unscaled\n",
    "\n",
    "    \n",
    "def cv_train(df_train_preprocessed: pd.DataFrame,\n",
    "             modeling: Dict[str, Any],\n",
    "             cv_splits_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
    "\n",
    "    model = {}\n",
    "    for pass_id in cv_splits_dict.keys():\n",
    "\n",
    "        # splitting\n",
    "        y = _split_train_val(df_train_preprocessed, cv_splits_dict, pass_id)  # cv_splits_dict[pass_id]\n",
    "\n",
    "        # training\n",
    "        model[pass_id] = ForecastingModel(y['train'], modeling).fit()   \n",
    "    \n",
    "    longest_pass_id = pass_id\n",
    "    return {\n",
    "        'intermediate_models': model,\n",
    "        'model': model[longest_pass_id]\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate(model, para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['pass_3'].predict(\n",
    "    start='2015-06-21',\n",
    "    end='2015-06-27',\n",
    "    scaler=preprocessing_pipeline,  # TODO: populate all districts columns, then predict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wind_stf",
   "language": "python",
   "name": "wind_stf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
